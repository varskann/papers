{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hardnet.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2cnYY3q6Ujp",
        "colab_type": "text"
      },
      "source": [
        "# [HarDNet: A Low Memory Traffic Network](https://arxiv.org/pdf/1909.00948.pdf)\n",
        "\n",
        "*    [Official Pytorch Implementation](https://github.com/PingoLH/Pytorch-HarDNet)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ueXZ97r431r",
        "colab_type": "text"
      },
      "source": [
        "### Setup [wandb](https://www.wandb.com/)\n",
        "\n",
        "\n",
        "> Wandb will be used for all the visualizations, hyperparameters tracking \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9xVVbZ8oTdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade --quiet wandb\n",
        "!wandb login ed0f2f2088e4953ef9392d1a2141681a565aec45"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyECwauIoxZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import wandb\n",
        "wandb.init(project=\"hardnet\", name=\"hardnet_classification_2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woRidjjxREJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --quiet pytorch_lightning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCO4x0aZ5zN4",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSC3FkHk6Dze",
        "colab_type": "text"
      },
      "source": [
        "### Module Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJwcebOpkKOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Imports\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "import warnings\n",
        "import sys\n",
        "import numpy as np\n",
        "from statistics import mean\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.distributed as dist\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import torch.nn.init as init\n",
        "from pytorch_lightning.metrics.functional import accuracy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm0Eg7gt7JyJ",
        "colab_type": "text"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "1. Download  \n",
        "2. Unzip / Format \n",
        "3. Pytorch Dataloader\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMWrXtwqghT7",
        "colab_type": "text"
      },
      "source": [
        "## Train/Validate Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIF7Og4DhKuS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Hyperparameters\n",
        "\n",
        "model_names = ['hardnet39ds', 'hardnet68ds', 'hardnet68', 'hardnet85']\n",
        "model_arch = model_names[0]\n",
        "lr = 0.001\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "hyperparams = dict(\n",
        "    workers = 8,\n",
        "    batch_size = 64,\n",
        "    max_epochs = 100,\n",
        "    start_epoch = 0,\n",
        "    learning_rate = lr,\n",
        "    momentum = 0.9,\n",
        "    weight_decay = 6e-5,\n",
        "    dropout = 0.5,\n",
        "    model_arch = model_arch,\n",
        "    depth_wise = 'ds' in model_arch,\n",
        "    pretrained = False,\n",
        "    dataset = \"pokemon\",\n",
        "    device = device\n",
        ")\n",
        "\n",
        "wandb.config.update(hyperparams)\n",
        "\n",
        "data = \"drive/My Drive/training/pokemon_data/\"\n",
        "wc = wandb.config\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy8l6pu9Jg1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
        "    \"\"\"Samples elements randomly from a given list of indices for imbalanced dataset\n",
        "    Arguments:\n",
        "        indices (list, optional): a list of indices\n",
        "        num_samples (int, optional): number of samples to draw\n",
        "        callback_get_label func: a callback-like function which takes two arguments - dataset and index\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, indices=None, num_samples=None, callback_get_label=None):\n",
        "                \n",
        "        # if indices is not provided, \n",
        "        # all elements in the dataset will be considered\n",
        "        self.indices = list(range(len(dataset))) \\\n",
        "            if indices is None else indices\n",
        "\n",
        "        # define custom callback\n",
        "        self.callback_get_label = callback_get_label\n",
        "\n",
        "        # if num_samples is not provided, \n",
        "        # draw `len(indices)` samples in each iteration\n",
        "        self.num_samples = len(self.indices) \\\n",
        "            if num_samples is None else num_samples\n",
        "            \n",
        "        # distribution of classes in the dataset \n",
        "        label_to_count = {}\n",
        "        for idx in self.indices:\n",
        "            label = self._get_label(dataset, idx)\n",
        "            if label in label_to_count:\n",
        "                label_to_count[label] += 1\n",
        "            else:\n",
        "                label_to_count[label] = 1\n",
        "                \n",
        "        # weight for each sample\n",
        "        weights = [1.0 / label_to_count[self._get_label(dataset, idx)]\n",
        "                   for idx in self.indices]\n",
        "        self.weights = torch.DoubleTensor(weights)\n",
        "\n",
        "    def _get_label(self, dataset, idx):\n",
        "        if isinstance(dataset, torchvision.datasets.MNIST):\n",
        "            return dataset.train_labels[idx].item()\n",
        "        elif isinstance(dataset, torchvision.datasets.ImageFolder):\n",
        "            return dataset.imgs[idx][1]\n",
        "        elif isinstance(dataset, torch.utils.data.Subset):\n",
        "            return dataset.dataset.imgs[idx][1]\n",
        "        elif self.callback_get_label:\n",
        "            return self.callback_get_label(dataset, idx)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "                \n",
        "    def __iter__(self):\n",
        "        return (self.indices[i] for i in torch.multinomial(\n",
        "            self.weights, self.num_samples, replacement=True))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FS6If4NWybp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_weights_for_balanced_classes(images, nclasses):                        \n",
        "    count = [0] * nclasses                                                      \n",
        "    for item in images:                                                         \n",
        "        count[item[1]] += 1                                                     \n",
        "    weight_per_class = [0.] * nclasses                                      \n",
        "    N = float(sum(count))                                                   \n",
        "    for i in range(nclasses):                                                   \n",
        "        weight_per_class[i] = N/float(count[i])                                 \n",
        "    weight = [0] * len(images)                                              \n",
        "    for idx, val in enumerate(images):                                          \n",
        "        weight[idx] = weight_per_class[val[1]]                                  \n",
        "    return weight  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ebW7XHNgq0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ki = None\n",
        "kj = None\n",
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    \"\"\"\n",
        "    Train an epoch\n",
        "    \"\"\"\n",
        "    global ki, kj\n",
        "    model.train() \n",
        "    epoch_loss = []\n",
        "    epoch_accuracy = []\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "        input = input.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        output = model(input)\n",
        "        ki = target\n",
        "        kj = output\n",
        "        loss = criterion(output, target)\n",
        "        epoch_loss.append(loss.item())\n",
        "        \n",
        "\n",
        "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "        epoch_accuracy.append(acc1.item())\n",
        "\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        wandb.log({\"train_loss_per_iteration\": loss.item(),\n",
        "                   \"train_accuracy_per_iteration\": acc1.item()})\n",
        "        break\n",
        "\n",
        "    wandb.log({\"train_loss_per_epoch\": mean(epoch_loss),\n",
        "               \"train_accuracy_per_epoch\": mean(epoch_accuracy)})\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmSNJrRHvwuS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(val_loader, model, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = []\n",
        "    epoch_accuracy = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (input, target) in enumerate(val_loader):\n",
        "            input = input.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            # compute output\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target)\n",
        "            epoch_loss.append(loss.item())\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "            epoch_accuracy.append(acc1.item())\n",
        "\n",
        "\n",
        "            wandb.log({\"val_loss_per_iteration\": loss.item(),\n",
        "                      \"val_accuracy_per_iteration\": acc1.item()})\n",
        "\n",
        "    wandb.log({\"val_loss_per_epoch\": mean(epoch_loss),\n",
        "               \"val_accuracy_per_epoch\": mean(epoch_accuracy)})\n",
        "\n",
        "    return mean(epoch_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJa0Cl0NxoEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
        "    torch.save(state, filename)\n",
        "    if is_best:\n",
        "        shutil.copyfile(filename, 'model_best.pth.tar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS4albtHxr8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def adjust_learning_rate(optimizer, epoch, lr):\n",
        "    #Cosine learning rate decay\n",
        "    lr = 0.5 * lr  * (1 + np.cos(np.pi * (epoch)/ wc.max_epochs ))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    return lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqChd3ZBxw8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(output, target, topk=(1,)):\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "        print(pred)\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AT5Xc5Zjxx49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weights_init(m):\n",
        "    for key in m.state_dict():\n",
        "        if key.split('.')[-1] == 'weight':\n",
        "            if 'conv' in key:\n",
        "                init.xavier_normal_(m.state_dict()[key])\n",
        "            if 'bn' in key:\n",
        "                m.state_dict()[key][...] = 1\n",
        "        elif key.split('.')[-1] == 'bias':\n",
        "            m.state_dict()[key][...] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to89R0CbvyOz",
        "colab_type": "text"
      },
      "source": [
        "## HardNet Network Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmaFzz0JxmNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    def forward(self, x):\n",
        "        return x.view(x.data.size(0),-1)\n",
        "\n",
        "\n",
        "\n",
        "class CombConvLayer(nn.Sequential):\n",
        "    def __init__(self, in_channels, out_channels, kernel=1, stride=1, dropout=0.1, bias=False):\n",
        "        super().__init__()\n",
        "        self.add_module('layer1',ConvLayer(in_channels, out_channels, kernel))\n",
        "        self.add_module('layer2',DWConvLayer(out_channels, out_channels, stride=stride))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return super().forward(x)\n",
        "\n",
        "class DWConvLayer(nn.Sequential):\n",
        "    def __init__(self, in_channels, out_channels,  stride=1,  bias=False):\n",
        "        super().__init__()\n",
        "        out_ch = out_channels\n",
        "        \n",
        "        groups = in_channels\n",
        "        kernel = 3\n",
        "        #print(kernel, 'x', kernel, 'x', out_channels, 'x', out_channels, 'DepthWise')\n",
        "        \n",
        "        self.add_module('dwconv', nn.Conv2d(groups, groups, kernel_size=3,\n",
        "                                          stride=stride, padding=1, groups=groups, bias=bias))\n",
        "        self.add_module('norm', nn.BatchNorm2d(groups))\n",
        "    def forward(self, x):\n",
        "        return super().forward(x)  \n",
        "\n",
        "class ConvLayer(nn.Sequential):\n",
        "    def __init__(self, in_channels, out_channels, kernel=3, stride=1, dropout=0.1, bias=False):\n",
        "        super().__init__()\n",
        "        out_ch = out_channels\n",
        "        groups = 1\n",
        "        #print(kernel, 'x', kernel, 'x', in_channels, 'x', out_channels)\n",
        "        self.add_module('conv', nn.Conv2d(in_channels, out_ch, kernel_size=kernel,          \n",
        "                                          stride=stride, padding=kernel//2, groups=groups, bias=bias))\n",
        "        self.add_module('norm', nn.BatchNorm2d(out_ch))\n",
        "        self.add_module('relu', nn.ReLU6(True))                                          \n",
        "    def forward(self, x):\n",
        "        return super().forward(x)\n",
        "\n",
        "\n",
        "class HarDBlock(nn.Module):\n",
        "    def get_link(self, layer, base_ch, growth_rate, grmul):\n",
        "        if layer == 0:\n",
        "          return base_ch, 0, []\n",
        "        out_channels = growth_rate\n",
        "        link = []\n",
        "        for i in range(10):\n",
        "          dv = 2 ** i\n",
        "          if layer % dv == 0:\n",
        "            k = layer - dv\n",
        "            link.append(k)\n",
        "            if i > 0:\n",
        "                out_channels *= grmul\n",
        "        out_channels = int(int(out_channels + 1) / 2) * 2\n",
        "        in_channels = 0\n",
        "        for i in link:\n",
        "          ch,_,_ = self.get_link(i, base_ch, growth_rate, grmul)\n",
        "          in_channels += ch\n",
        "        return out_channels, in_channels, link\n",
        "\n",
        "    def get_out_ch(self):\n",
        "        return self.out_channels\n",
        "\n",
        "    def __init__(self, in_channels, growth_rate, grmul, n_layers, keepBase=False, residual_out=False, dwconv=False):\n",
        "        super().__init__()\n",
        "        self.keepBase = keepBase\n",
        "        self.links = []\n",
        "        layers_ = []\n",
        "        self.out_channels = 0 # if upsample else in_channels\n",
        "        for i in range(n_layers):\n",
        "          outch, inch, link = self.get_link(i+1, in_channels, growth_rate, grmul)\n",
        "          self.links.append(link)\n",
        "          use_relu = residual_out\n",
        "          if dwconv:\n",
        "            layers_.append(CombConvLayer(inch, outch))\n",
        "          else:\n",
        "            layers_.append(ConvLayer(inch, outch))\n",
        "          \n",
        "          if (i % 2 == 0) or (i == n_layers - 1):\n",
        "            self.out_channels += outch\n",
        "        #print(\"Blk out =\",self.out_channels)\n",
        "        self.layers = nn.ModuleList(layers_)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        layers_ = [x]\n",
        "        \n",
        "        for layer in range(len(self.layers)):\n",
        "            link = self.links[layer]\n",
        "            tin = []\n",
        "            for i in link:\n",
        "                tin.append(layers_[i])\n",
        "            if len(tin) > 1:            \n",
        "                x = torch.cat(tin, 1)\n",
        "            else:\n",
        "                x = tin[0]\n",
        "            out = self.layers[layer](x)\n",
        "            layers_.append(out)\n",
        "            \n",
        "        t = len(layers_)\n",
        "        out_ = []\n",
        "        for i in range(t):\n",
        "          if (i == 0 and self.keepBase) or \\\n",
        "             (i == t-1) or (i%2 == 1):\n",
        "              out_.append(layers_[i])\n",
        "        out = torch.cat(out_, 1)\n",
        "        return out\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "class HarDNet(nn.Module):\n",
        "    def __init__(self, depth_wise=False, arch=85, pretrained=True, weight_path=''):\n",
        "        super().__init__()\n",
        "        first_ch  = [32, 64]\n",
        "        second_kernel = 3\n",
        "        max_pool = True\n",
        "        grmul = 1.7\n",
        "        drop_rate = 0.1\n",
        "        num_outputs = 150\n",
        "        \n",
        "        #HarDNet68\n",
        "        ch_list = [  128, 256, 320, 640, 1024]\n",
        "        gr       = [  14, 16, 20, 40,160]\n",
        "        n_layers = [   8, 16, 16, 16,  4]\n",
        "        downSamp = [   1,  0,  1,  1,  0]\n",
        "        \n",
        "        if arch==85:\n",
        "          #HarDNet85\n",
        "          first_ch  = [48, 96]\n",
        "          ch_list = [  192, 256, 320, 480, 720, 1280]\n",
        "          gr       = [  24,  24,  28,  36,  48, 256]\n",
        "          n_layers = [   8,  16,  16,  16,  16,   4]\n",
        "          downSamp = [   1,   0,   1,   0,   1,   0]\n",
        "          drop_rate = 0.2\n",
        "        elif arch==39:\n",
        "          #HarDNet39\n",
        "          first_ch  = [24, 48]\n",
        "          ch_list = [  96, 320, 640, 1024]\n",
        "          grmul = 1.6\n",
        "          gr       = [  16,  20, 64, 160]\n",
        "          n_layers = [   4,  16,  8,   4]\n",
        "          downSamp = [   1,   1,  1,   0]\n",
        "          \n",
        "        if depth_wise:\n",
        "          second_kernel = 1\n",
        "          max_pool = False\n",
        "          drop_rate = 0.05\n",
        "        \n",
        "        blks = len(n_layers)\n",
        "        self.base = nn.ModuleList([])\n",
        "\n",
        "        # First Layer: Standard Conv3x3, Stride=2\n",
        "        self.base.append (\n",
        "             ConvLayer(in_channels=3, out_channels=first_ch[0], kernel=3,\n",
        "                       stride=2,  bias=False) )\n",
        "  \n",
        "        # Second Layer\n",
        "        self.base.append ( ConvLayer(first_ch[0], first_ch[1],  kernel=second_kernel) )\n",
        "        \n",
        "        # Maxpooling or DWConv3x3 downsampling\n",
        "        if max_pool:\n",
        "          self.base.append(nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "        else:\n",
        "          self.base.append ( DWConvLayer(first_ch[1], first_ch[1], stride=2) )\n",
        "\n",
        "        # Build all HarDNet blocks\n",
        "        ch = first_ch[1]\n",
        "        for i in range(blks):\n",
        "            blk = HarDBlock(ch, gr[i], grmul, n_layers[i], dwconv=depth_wise)\n",
        "            ch = blk.get_out_ch()\n",
        "            self.base.append ( blk )\n",
        "            \n",
        "            if i == blks-1 and arch == 85:\n",
        "                self.base.append ( nn.Dropout(0.1))\n",
        "            \n",
        "            self.base.append ( ConvLayer(ch, ch_list[i], kernel=1) )\n",
        "            ch = ch_list[i]\n",
        "            if downSamp[i] == 1:\n",
        "              if max_pool:\n",
        "                self.base.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "              else:\n",
        "                self.base.append ( DWConvLayer(ch, ch, stride=2) )\n",
        "            \n",
        "        \n",
        "        ch = ch_list[blks-1]\n",
        "        self.base.append (\n",
        "            nn.Sequential(\n",
        "                nn.AdaptiveAvgPool2d((1,1)),\n",
        "                Flatten(),\n",
        "                nn.Dropout(drop_rate),\n",
        "                nn.Linear(ch, num_outputs) ))\n",
        "                \n",
        "        \n",
        "        if pretrained:\n",
        "          if hasattr(torch, 'hub'):\n",
        "          \n",
        "            if arch == 68 and not depth_wise:\n",
        "              checkpoint = 'https://ping-chao.com/hardnet/hardnet68-5d684880.pth'\n",
        "            elif arch == 85 and not depth_wise:\n",
        "              checkpoint = 'https://ping-chao.com/hardnet/hardnet85-a28faa00.pth'\n",
        "            elif arch == 68 and depth_wise:\n",
        "              checkpoint = 'https://ping-chao.com/hardnet/hardnet68ds-632474d2.pth'\n",
        "            else:\n",
        "              checkpoint = 'https://ping-chao.com/hardnet/hardnet39ds-0e6c6fa9.pth'\n",
        "\n",
        "            self.load_state_dict(torch.hub.load_state_dict_from_url(checkpoint, progress=False))\n",
        "          else:\n",
        "            postfix = 'ds' if depth_wise else ''\n",
        "            weight_file = '%shardnet%d%s.pth'%(weight_path, arch, postfix)            \n",
        "            if not os.path.isfile(weight_file):\n",
        "              print(weight_file,'is not found')\n",
        "              exit(0)\n",
        "            weights = torch.load(weight_file)\n",
        "            self.load_state_dict(weights)\n",
        "          \n",
        "          postfix = 'DS' if depth_wise else ''\n",
        "          print('ImageNet pretrained weights for HarDNet%d%s is loaded'%(arch, postfix))\n",
        "          \n",
        "    def forward(self, x):\n",
        "        for layer in self.base:\n",
        "          x = layer(x)\n",
        "        return x\n",
        "        \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m72C1kLxvxX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def hardnet68(pretrained=False, **kwargs):\n",
        "    \"\"\" # This docstring shows up in hub.help()\n",
        "    Harmonic DenseNet 68 model\n",
        "    pretrained (bool): kwargs, load pretrained weights into the model\n",
        "    \"\"\"\n",
        "    # Call the model, load pretrained weights\n",
        "    model = HarDNet(depth_wise=False, arch=68, pretrained=pretrained)\n",
        "    return model\n",
        "\n",
        "def hardnet85(pretrained=False, **kwargs):\n",
        "    \"\"\" # This docstring shows up in hub.help()\n",
        "    Harmonic DenseNet 85 model\n",
        "    pretrained (bool): kwargs, load pretrained weights into the model\n",
        "    \"\"\"\n",
        "    # Call the model, load pretrained weights\n",
        "    model = HarDNet(depth_wise=False, arch=85, pretrained=pretrained)\n",
        "    return model\n",
        "\n",
        "def hardnet68ds(pretrained=False, **kwargs):\n",
        "    \"\"\" # This docstring shows up in hub.help()\n",
        "    Harmonic DenseNet 68ds (Depthwise Separable) model\n",
        "    pretrained (bool): kwargs, load pretrained weights into the model\n",
        "    \"\"\"\n",
        "    # Call the model, load pretrained weights\n",
        "    model = HarDNet(depth_wise=True, arch=68, pretrained=pretrained)\n",
        "    return model\n",
        "\n",
        "def hardnet39ds(pretrained=False, **kwargs):\n",
        "    \"\"\" # This docstring shows up in hub.help()\n",
        "    Harmonic DenseNet 68ds (Depthwise Separable) model\n",
        "    pretrained (bool): kwargs, load pretrained weights into the model\n",
        "    \"\"\"\n",
        "    # Call the model, load pretrained weights\n",
        "    model = HarDNet(depth_wise=True, arch=39, pretrained=pretrained)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzlgOWjOzjZF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Load model and Start Training\n",
        "is_best = False\n",
        "best_acc1 = 0.0\n",
        "model = HarDNet(wc.depth_wise, wc.model_arch, pretrained=wc.pretrained)\n",
        "wandb.watch(model)\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "\n",
        "# define loss function (criterion) and optimizer\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr,\n",
        "                            momentum=wc.momentum,\n",
        "                            nesterov=True,\n",
        "                            weight_decay=wc.weight_decay)\n",
        "\n",
        "model.apply(weights_init)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print( \"Parameters=\", total_params )\n",
        "cudnn.benchmark = True\n",
        "\n",
        "# Data loading code\n",
        "traindir = os.path.join(data)\n",
        "valdir = os.path.join(data)\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                              std=[0.229, 0.224, 0.225])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(traindir,\n",
        "                                     transforms.Compose([\n",
        "                                          transforms.RandomResizedCrop(224),\n",
        "                                          transforms.RandomHorizontalFlip(),\n",
        "                                          transforms.ToTensor(),\n",
        "                                          normalize,\n",
        "                                      ]))\n",
        "\n",
        "# For unbalanced dataset we create a weighted sampler                       \n",
        "weights = make_weights_for_balanced_classes(train_dataset.imgs, len(train_dataset.classes))                                                                \n",
        "weights = torch.DoubleTensor(weights)                                       \n",
        "train_sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights)) \n",
        "\n",
        "# train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "                                           batch_size=wc.batch_size, \n",
        "                                           num_workers=wc.workers, \n",
        "                                           pin_memory=True,\n",
        "                                           sampler=train_sampler)\n",
        "\n",
        "val_dataset = datasets.ImageFolder(valdir, \n",
        "                                   transforms.Compose([\n",
        "                                   transforms.Resize(256),\n",
        "                                   transforms.CenterCrop(224),\n",
        "                                   transforms.ToTensor(),\n",
        "                                   normalize,\n",
        "                                ]))\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, \n",
        "                                         batch_size=wc.batch_size, \n",
        "                                         shuffle=False,\n",
        "                                         num_workers=wc.workers, \n",
        "                                         pin_memory=True)\n",
        "\n",
        "# validate(val_loader, model, criterion)\n",
        "# return\n",
        "\n",
        "for epoch in range(wc.start_epoch, wc.max_epochs):\n",
        "  # train_sampler.set_epoch(epoch)\n",
        "  lr = adjust_learning_rate(optimizer, epoch, lr)\n",
        "  wandb.log({\"learning_rate\": lr})\n",
        "\n",
        "  train(train_loader, model, criterion, optimizer, epoch)\n",
        "\n",
        "  acc1 = validate(val_loader, model, criterion)\n",
        "\n",
        "  # remember best acc@1 and save checkpoint\n",
        "  is_best = acc1 > best_acc1\n",
        "  best_acc1 = max(acc1, best_acc1)\n",
        "\n",
        "  save_checkpoint({\n",
        "      'epoch': epoch + 1,\n",
        "      'arch': model_arch,\n",
        "      'state_dict': model.state_dict(),\n",
        "      'best_acc1': best_acc1,\n",
        "      'optimizer' : optimizer.state_dict(),\n",
        "  }, is_best)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGnCF6ocBew1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchviz import make_dot\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0uaUuIEbiWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install torchviz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II8vgaNCaVOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.zeros(1, 3, 224, 224, dtype=torch.float, requires_grad=False).to(wc.device)\n",
        "out = model(x)\n",
        "make_dot(out)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoMDoafpbXU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}